
//
//  ViewController.swift
//  tensorflowiOS
//
//  Created by Chris Sharp on 11/10/17.
//  Copyright Â© 2017 Chris Sharp. All rights reserved.
//

import UIKit
import AVFoundation
import AudioToolbox


class DetectionController:UIViewController, AVCaptureVideoDataOutputSampleBufferDelegate
{
//    @IBOutlet weak var cameraUnavailableLabel : UILabel!
//    @IBOutlet weak var boundingBoxView        : BoundingBoxView!
    @IBOutlet weak var cameraPreviewView      : CameraPreviewView!
    var tensorflowGraph:TensorflowGraph? = nil

    override func viewDidLoad()
    {
        super.viewDidLoad()

        //
        // Configure the video preview.  We will grab frames
        // from the video preview and feed them into the tensorflow graph.
        // Then bounding boxes can be rendered onto the boundingBoxView.
        //
        cameraPreviewView.configureSession(delegate: self)
    }
    
    
    override func viewWillAppear(_ animated: Bool)
    {
        super.viewWillAppear(animated)

        //
        // Listen for the start of the AVSession.  This will signal the start
        // of the delivery of video frames and will trigger the
        // initialization of the tensorflow graph
        //
        NotificationCenter.default.addObserver(self, selector: #selector(OnAvSessionStarted(notification:)),
                                               name: NSNotification.Name(rawValue: kAVSessionStarted),
                                               object: nil)
    
        //
        // Also Listen for Session initialization failure or for when
        // the user doesn't authorize the use of the camera
        //
        NotificationCenter.default.addObserver(self, selector: #selector(OnSetupResultCameraNotAuthorized(notification:)),
                                               name: Notification.Name(kSetupResultCameraNotAuthorized),
                                               object:nil)

        NotificationCenter.default.addObserver(self, selector: #selector(OnSetupResultSessionConfigurationFailed(notification:)),
                                               name: Notification.Name(kSetupResultSessionConfigurationFailed),
                                               object:nil)
        
        //
        // Respond to the tensorflow graph's update of predictions.  This will
        // trigger the redrawing of the bounding boxes.
        //
        NotificationCenter.default.addObserver(self, selector: #selector(OnPredictionsUpdated(notification:)),
                                               name: Notification.Name(kPredictionsUpdated),
                                               object:nil)
        //
        // Start the AV Session. This will prompt the user for
        // permission to use the camera to present a video preview.
        //
        cameraPreviewView.startSession()
    }
    
    //
    // when the view disappears we shut down the session.  It will be restarted in ViewWillAppear
    //
    override func viewWillDisappear(_ animated: Bool)
    {
        super.viewWillAppear(animated)
        cameraPreviewView.stopSession()        
    }
    
    //
    // We allow autorotation to all orientations, but may have to rotate the
    // pixel buffer when we run the graph.
    //
    override var shouldAutorotate: Bool
    {
        return true
    }
    
    override var supportedInterfaceOrientations:UIInterfaceOrientationMask
    {
        return UIInterfaceOrientationMask.all
    }
    
    //
    // Override viewWillTransitionToSize so that we can update the videoPreviewLayer with the new orientation.
    //
    override func viewWillTransition(to size: CGSize, with coordinator: UIViewControllerTransitionCoordinator)
    {
        //
        // call super so the coordinator can be passed on
        // to views and child view controllers.
        //
        super.viewWillTransition(to: size, with: coordinator)
        
//        if let videoPreviewLayerConnection = cameraPreviewView.videoPreviewLayer.connection
//        {
//            //
//            // Change the orientation of the video session
//            //
//            let deviceOrientation = UIDevice.current.orientation
//            if let newVideoOrientation = AVCaptureVideoOrientation(deviceOrientation: deviceOrientation) {
//                videoPreviewLayerConnection.videoOrientation = newVideoOrientation
//            }
//        }
    }

    //
    // This delegate method is where we are notified of a new video frame.  We obtain
    // CVPixelBuffer from the provided sample buffer and pass it on to the tensorflow graph
    //
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection)
    {
        //
        // TensorflowGraph needs a CVPixelBuffer.  Get it from the sampleBuffer
        //
        let pixelBuffer: CVPixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)!

        //
        // if the graph is ready pass it on.
        //
        if tensorflowGraph != nil
        {
            tensorflowGraph?.runModel(on: pixelBuffer, orientation: UIDevice.current.orientation)
        }
    }
    
    
    func captureOutput(_ output: AVCaptureOutput, didDrop sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection)
    {
        //do something with dropped frames here
    }
        
    ////////////////////////////////////////////////////////////////////////////////////////////////////////////
    // MARK: - Notification Handlers
    
    @objc func OnAvSessionStarted(notification: NSNotification)
    {
        // Now that the user has granted permission to the camera
        // and we have a video session we can initialize our graph.
        tensorflowGraph = TensorflowGraph()
    }

    @objc func OnSetupResultCameraNotAuthorized(notification: NSNotification)
    {
        DispatchQueue.main.async {
            let changePrivacySetting = "Please grant permission to use the camera in Settings"
            let message = NSLocalizedString(changePrivacySetting, comment: "Alert message when we have no access to the camera")
            let alertController = UIAlertController(title: "TensorflowiOS", message: message, preferredStyle: .alert)
            
            alertController.addAction(UIAlertAction(title: NSLocalizedString("OK", comment: "Alert OK button"),
                                                    style: .cancel,
                                                    handler: nil))
            
            alertController.addAction(UIAlertAction(title: NSLocalizedString("Settings", comment: "Button to open Settings"),
                                                    style: .`default`,
                                                    handler: { _ in
                                                        UIApplication.shared.open(URL(string: UIApplicationOpenSettingsURLString)!, options: [:], completionHandler: nil)
            }))
            
            self.present(alertController, animated: true, completion: nil)
        }
    }

    @objc func OnSetupResultSessionConfigurationFailed(notification: NSNotification)
    {
        DispatchQueue.main.async {
            let alertMsg = "Something went wrong during capture session configuration"
            let message = NSLocalizedString("Unable to capture media", comment: alertMsg)
            let alertController = UIAlertController(title: "TensorflowiOS", message: message, preferredStyle: .alert)
            
            alertController.addAction(UIAlertAction(title: NSLocalizedString("OK", comment: "OK button"),
                                                    style: .cancel,
                                                    handler: nil))
            
            self.present(alertController, animated: true, completion: nil)
        }
    }

    @objc func OnPredictionsUpdated(notification: NSNotification)
    {
        DispatchQueue.main.async {
            if let userinfo = notification.userInfo {
                if let predictions:[TensorflowPrediction] = userinfo["predictions"] as? [TensorflowPrediction] {
                    //
                    // Update the Bounding boxes and labels from the
                    // new predictions coming out of the graph.
                    //
                    
                    for pred: TensorflowPrediction in predictions {
                        print(pred.label!, pred.score)
                    }
                    
//                    self.boundingBoxView.updateBoundingBoxes(predictions)
                }
            }
        }
    }
}

////////////////////////////////////////////////////////////////////
// MARK: - AVCaptureVideoOrientation extension

extension AVCaptureVideoOrientation {
    init?(deviceOrientation: UIDeviceOrientation) {
        switch deviceOrientation {
        case .portrait:           self = .portrait
        case .portraitUpsideDown: self = .portraitUpsideDown
        case .landscapeLeft:      self = .landscapeRight
        case .landscapeRight:     self = .landscapeLeft
        default: return nil
        }
    }
    
    init?(interfaceOrientation: UIInterfaceOrientation) {
        switch interfaceOrientation {
        case .portrait:           self = .portrait
        case .portraitUpsideDown: self = .portraitUpsideDown
        case .landscapeLeft:      self = .landscapeLeft
        case .landscapeRight:     self = .landscapeRight
        default: return nil
        }
    }
}

